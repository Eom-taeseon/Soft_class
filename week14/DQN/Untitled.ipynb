{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = Dense(24, activation = 'relu')\n",
    "        self.fc2 = Dense(24, activation = 'relu')\n",
    "        self.fc_out = Dense(action_size, kernel_initializer = Randomuniform(-1e-3, 1e-3))\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.fc_out(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action):\n",
    "        self.render = False\n",
    "        \n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        print(\"39----state_size : \", state_size)\n",
    "        print(\"40----action_size : \", action_size)\n",
    "        \n",
    "        # DQN 하이퍼 파리미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        \n",
    "        # 리플레이 메모리, 최대 크기\n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        \n",
    "        # 모델과 타깃 모델 생성\n",
    "        self.model = DQN(action_size)\n",
    "        self.target_model = DQN(action_size)\n",
    "        self.optimizer = Adam(lr=self.learning_rate)\n",
    "        \n",
    "        # 타깃 모델 초기화\n",
    "        self.update_target_model()\n",
    "        \n",
    "    # 샘플 <s, a, r, s`>을 리플레이 메모리에 저장\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        print(\"78----state, action, reward, next_state, done : \", state, action, reward, next_state, done)\n",
    "        \n",
    "    def trina_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            print(\"84----self.epsilon : \", self.epsilon)\n",
    "            \n",
    "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "        print(\"89----mini_batch : \", mini_batch)\n",
    "        \n",
    "        states = np.array([sample[0][0] for sample in mini_batch])\n",
    "        actions = np.array([sample[1] for sample in mini_batch])\n",
    "        rewards = np.array([sample[2] for sample in mini_batch])\n",
    "        next_states = np.array([sample[3][0] for sample in mini_batch])\n",
    "        dones = np.array([sample[4] for sample in mini_batch])\n",
    "        print(\"91----states : \", satates)\n",
    "        print(\"92----actions : \", actions)\n",
    "        print(\"93----rewards : \", rewards)\n",
    "        print(\"94----next_states : \", next_states)\n",
    "        print(\"95----dones : \", dones)\n",
    "        \n",
    "        # 학습 파라메터\n",
    "        model_params = self.model.trainable_variables\n",
    "        print(\"102----model_params : \", model_params)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 현재 상태에 대한 모델의 큐함수\n",
    "            predicts = self.model(states)\n",
    "            one_hot_action = tf.one_hot(actions, self.action_size)\n",
    "            prediects = tf.reduce_sum(one_hot_action * predicts, axis = 1)\n",
    "            print(\"106----predicts : \", predicts)\n",
    "            print(\"107----one_hot_action : \", one_hot_action)\n",
    "            print(\"108----predicts : \", predicts)\n",
    "            \n",
    "            # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "            target_predicts = self.target_model(next_states)\n",
    "            target_predicts = tf.stop_gradient(target_predicts)\n",
    "            print(\"114----target_predicts : \", target_predicts)\n",
    "            \n",
    "            # 벨만 최적 방정식을 이용한 어벧이트 타깃\n",
    "            max_q = np.amax(target_predicts, axis = -1)\n",
    "            targets = rewards + (1-dones)*self.discount_factor + max_q\n",
    "            loss = tf.reduce_mean(tf.square(targets - predicts))\n",
    "            print(\"120----max_q : \", max_q)\n",
    "            print(\"121----targets : \", targets)\n",
    "            print(\"122----loss : \", loss)\n",
    "            \n",
    "        # 오류함수를 줄이는 방향으로 모델 업데이트\n",
    "        grads = tape.gradient(loss, model_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "        print(\"128----grads : \", grads)\n",
    "        print(\"129----self.optimizer.apply_gradients : \", self.optimizer.apply_gradients)\n",
    "        \n",
    "    def dupate_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        print(\"64----self.model.get_weights() : \", self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # CartPole-v1 환경, 최대 타임스텝 수가 500\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    print(\"137----state_size : \", state_size)\n",
    "    print(\"138----action_size : \", )\n",
    "    \n",
    "    # DQN 에이전트 생성\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    \n",
    "    scores, episodes = [], []\n",
    "    score_avg = 0\n",
    "    \n",
    "    num_episode = 10\n",
    "    \n",
    "    for e in range(num_episodesode):\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        # env 초기화\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        print(\"153----state : \", state)\n",
    "        \n",
    "        while not doen:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            print(\"160----state : \", state)\n",
    "            \n",
    "            # 현재 상태로 행동을 선택\n",
    "            action = agent.get_action(state)\n",
    "            print(\"161----action : \", action)\n",
    "            \n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            print(\"164----next_state, reward, done, info : \", state, reward, done, info)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            print(\"166----next_sate : \", next_sate)\n",
    "            \n",
    "            # 타임스텝마다 보상0.1, 에피소드가 중간에 끝나면 -1 보상\n",
    "            score += reward\n",
    "            print(\"170----reward, state : \", reward, state)\n",
    "            reward = 0.1 if not done or score == 500 else -1\n",
    "            print(\"172----reward : \", reward)\n",
    "            \n",
    "            # 리플레이 메모리에 샘플 <s, a, r, s`> 저장\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            \n",
    "            print(\"len(agent.memory) : \", len(agent.memory))\n",
    "            print(\"agent.train_start : \", agent.train_start)\n",
    "            \n",
    "            if len(agent.memory) >= agent.train_start:\n",
    "                agent.train_model()\n",
    "            \n",
    "            if done:\n",
    "                # 각 에피소드마다 타깃 모델을 모델의 가중치로 업데이트\n",
    "                agent.update_target_model()\n",
    "                \n",
    "                # 에피소드마다 학습 결과 출력\n",
    "                score_avg = 0.9*score_avg + 0.1*score if score_avg != 0 else score\n",
    "                print(\"episode : {:3d} | score avg : {:3.2f} | memory length : {:4d} | epsilon : {:.4f}\".format(\n",
    "                e, score_avg, len(agent.momory), agent.epsilon))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
