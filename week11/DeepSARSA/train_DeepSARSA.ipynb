{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import environment as Env\n",
    "import keras\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥살사 인공신경망\n",
    "class DeepSARSA(tf.keras.Model):\n",
    "    def __init__(self, action_size):\n",
    "        super(DeepSARSA, self).__init__()\n",
    "        self.fc1 = keras.layers.Dense(30, activation = 'relu') # 입력층 (unit 개수 : 30, 활성함수 : ReLU)\n",
    "        self.fc2 = keras.layers.Dense(30, activation = 'relu') # 은닉층 (unit 개수 : 30, 활성함수 : ReLU)\n",
    "        self.fc_out = keras.layers.Dense(action_size) # 출력층 (action_size = 5, 상, 하, 좌, 우, 제자리)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.fc_out(x)\n",
    "        \n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리드월드 예제에서의 딥살사 에이전트\n",
    "class DeepSARSAAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # 상태의 크기와 행동의 크기 정의\n",
    "        self.state_size = state_size # 상태의 크기 정의\n",
    "        self.action_size = action_size # 행동의 크기 정의\n",
    "        \n",
    "        # 딥살사 하이퍼 파라메터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_decay = .9999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.model = DeepSARSA(self.action_size)\n",
    "        self.optimizer = keras.optimizers.Adam(lr = self.learning_rate)\n",
    "        \n",
    "    # 엡실론 탐욕 정택으로 행동 선택\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            #print('get_action = epsilon :', self.epsilon)\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        else:\n",
    "            q_values = self.model(state)\n",
    "            #print(\"get_action - q_values : \", q_values)\n",
    "            return np.argmax(q_values[0])\n",
    "        \n",
    "    def train_model(self, state, action, reward, next_state, next_action, done): # epsilon 값이 지정한 최소값보다 큰 경우에 감소시킴\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        # 학습 파라메터\n",
    "        model_params = self.model.trainable_variables\n",
    "        \n",
    "        with  tf.GradientTape() as tape:\n",
    "            tape.watch(model_params)\n",
    "            predict = self.model(state)[0] # 상태를 모델의 입력으로 하여 각 행동에 대한 큐함수 값을 받음\n",
    "            #print(\"predict : \", predict)\n",
    "            one_hot_action = tf.one_hot([action], self.action_size) # 행동을 one hot 인코딩으로 변환\n",
    "            #print(\"one_hot_action : \", one_hot_action)\n",
    "            predict = tf.reduce_sum(one_hot_action * predict, axis = 1) # 현재 행동의 큐함수 값 선택\n",
    "            #print(\"predict : \", predict)\n",
    "            \n",
    "            # done = True일 경우 에피소드가 끝나서 다음 상태가 없음\n",
    "            next_q = self.model(next_state)[0][next_action] # 다음 상태를 입력해 다음 행동에 대한 큐함수를 받음\n",
    "            #print(\"0. next_q : \", next_q)\n",
    "            \n",
    "            target = reward + (1 - done) * self.discount_factor * next_q # DeepSARSA 공식 적용\n",
    "            #print(\"10. reward : \", reward)\n",
    "            #print(\"11. discount factor : \", self.discount_factor)\n",
    "            #print(\"12. target : \", target)\n",
    "            \n",
    "            # MSE 오류 함수 계산\n",
    "            loss = tf.reduce_mean(tf.square(target - predict))\n",
    "            \n",
    "        # 오류 함수를 줄이는 방향으로 모델 업데이트\n",
    "        grads = tape.gradient(loss, model_params) # gradient 계산\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params)) # gradient를 줄이는 방향으로 가중치 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :  1\n",
      "step :  37\n",
      "score :  -3\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  2\n",
      "step :  43\n",
      "score :  -2\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  3\n",
      "step :  325\n",
      "score :  -46\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  4\n",
      "step :  109\n",
      "score :  -16\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  5\n",
      "step :  175\n",
      "score :  -21\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  6\n",
      "step :  230\n",
      "score :  -21\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  7\n",
      "step :  102\n",
      "score :  -14\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  8\n",
      "step :  205\n",
      "score :  -24\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  9\n",
      "step :  143\n",
      "score :  -5\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  10\n",
      "step :  46\n",
      "score :  -4\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  11\n",
      "step :  263\n",
      "score :  -17\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  12\n",
      "step :  48\n",
      "score :  -4\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  13\n",
      "step :  48\n",
      "score :  -6\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  14\n",
      "step :  78\n",
      "score :  -6\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  15\n",
      "step :  96\n",
      "score :  -6\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  16\n",
      "step :  145\n",
      "score :  -11\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  17\n",
      "step :  65\n",
      "score :  -5\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  18\n",
      "step :  23\n",
      "score :  -2\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  19\n",
      "step :  88\n",
      "score :  -3\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  20\n",
      "step :  51\n",
      "score :  -3\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  21\n",
      "step :  10\n",
      "score :  -1\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  22\n",
      "step :  40\n",
      "score :  -1\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  23\n",
      "step :  23\n",
      "score :  0\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  24\n",
      "step :  121\n",
      "score :  -15\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  25\n",
      "step :  108\n",
      "score :  -9\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  26\n",
      "step :  76\n",
      "score :  -2\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  27\n",
      "step :  47\n",
      "score :  -4\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  28\n",
      "step :  23\n",
      "score :  1\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  29\n",
      "step :  41\n",
      "score :  0\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  30\n",
      "step :  38\n",
      "score :  -2\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  31\n",
      "step :  33\n",
      "score :  -1\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  32\n",
      "step :  129\n",
      "score :  -6\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  33\n",
      "step :  31\n",
      "score :  -1\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  34\n",
      "step :  108\n",
      "score :  -11\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  35\n",
      "step :  58\n",
      "score :  -1\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  36\n",
      "step :  102\n",
      "score :  -12\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  37\n",
      "step :  41\n",
      "score :  -2\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  38\n",
      "step :  43\n",
      "score :  -6\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  39\n",
      "step :  19\n",
      "score :  -3\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  40\n",
      "step :  156\n",
      "score :  -18\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  41\n",
      "step :  56\n",
      "score :  -3\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  42\n",
      "step :  42\n",
      "score :  -2\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  43\n",
      "step :  95\n",
      "score :  -7\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  44\n",
      "step :  115\n",
      "score :  -10\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  45\n",
      "step :  98\n",
      "score :  -7\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Episode :  46\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 환경과 에이전트 생성\n",
    "    env = Env.Env(render_speed = 0.01) # 환경 instance 생성 (게임 속도를 0.01로 조정)\n",
    "    state_size = 15 # 상태 개수 정의\n",
    "    action_space = [0, 1, 2, 3, 4] # 행동 정의\n",
    "    action_size = len(action_space) # 행동 개수 정의\n",
    "    agent = DeepSARSAAgent(state_size, action_size) # DeepSARSAAgnet instance 생성\n",
    "    \n",
    "    scores, episodes = [], []\n",
    "    \n",
    "    EPISODES = 100 # episode 횟수 정의.\n",
    "    \n",
    "    for e in range(EPISODES): \n",
    "        print(\"Episode : \", e + 1)\n",
    "        done = False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        \n",
    "        # env 초기화\n",
    "        #print(\"-- entering env.reset()\")\n",
    "        state = env.reset() # 환경을 초기화하고 상태를 받음 (list 형식)\n",
    "        #print(\"-- exit env.reset()\")\n",
    "        state = np.reshape(state, [1, state_size]) # 상태 list를 (1, 15)의 numpy.array로 변환\n",
    "        \n",
    "        while not done: # episode가 끝나지 않으면 계속 실행\n",
    "            # 몇 번째 스텝인지 확인\n",
    "            step += 1\n",
    "            \n",
    "            # 현재 상태에 대한 행동 선택\n",
    "            #print(\"-- entering get_action()\")\n",
    "            action = agent.get_action(state)\n",
    "            #print(\"action : \", action)\n",
    "            #print(\"-- exit get_action()\")\n",
    "            \n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행 후 샘플 수집\n",
    "            #print(\"-- entering env.step()\")\n",
    "            #print(\"state : \", state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            #print(\"-- exit env.step()\")\n",
    "            #print(\"10. next_state : \", next_state)\n",
    "            #print(\"11. reward : \", reward)\n",
    "            next_state = np.reshape(next_state, [1, state_size])            \n",
    "            #print(\"11. next_state : \", next_state)\n",
    "            next_action = agent.get_action(next_state)\n",
    "            \n",
    "            # 샘플로 모델 학습\n",
    "            agent.train_model(state, action, reward, next_state, next_action, done)\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            \n",
    "        print(\"step : \", step)\n",
    "        print(\"score : \", score)\n",
    "        print(\"\\n~~~~~~~~~~\\n\")\n",
    "        \n",
    "quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"end!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
