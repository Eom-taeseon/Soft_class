{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Environment.ipynb\n"
     ]
    }
   ],
   "source": [
    "from Environment import Environment\n",
    "import numpy as np\n",
    "import random\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    \n",
    "    def __init__(self, args, env):\n",
    "        # set hyperparameters\n",
    "        self.max_episodes = int(args.max_episodes)\n",
    "        self.max_actions = int(args.max_actions)\n",
    "        self.learning_rate = float(args.learning_rate)\n",
    "        self.discount = float(args.discount)\n",
    "        self.exploration_rate = float(args.exploration_rate)\n",
    "        self.exploration_decay = 1.0/float(args.max_episodes)\n",
    "        \n",
    "        # get environmnet\n",
    "        self.env = env\n",
    "        \n",
    "        # initialize Q(s, a)\n",
    "        row = env.observation_space.n\n",
    "        col = env.action_space.n\n",
    "        self.Q = np.zeros((row, col))\n",
    "    \n",
    "    def _policy(self, mode, state, e_rate=0):\n",
    "        if mode=='train':\n",
    "            if random.random() > e_rate:\n",
    "                return np.argmax(self.Q[state,:]) # exploitation\n",
    "            else:\n",
    "                return self.env.action_space.sample() # exploration\n",
    "        elif mode=='test':\n",
    "            return np.argmax(self.Q[state,:])\n",
    "    \n",
    "    def train(self):\n",
    "        # get hyper-parameters\n",
    "        max_episodes = self.max_episodes\n",
    "        max_actions = self.max_actions\n",
    "        learning_rate = self.learning_rate\n",
    "        discount = self.discount\n",
    "        exploration_rate = self.exploration_rate\n",
    "        exploration_decay = 1.0/self.max_episodes\n",
    "        \n",
    "        # reset Q for initialize\n",
    "        row = self.env.observation_space.n\n",
    "        col = self.env.action_space.n\n",
    "        self.Q = np.zeros((row, col))\n",
    "\n",
    "        # start training\n",
    "        for i in range(max_episodes):\n",
    "            state = self.env.reset() # reset the environment per eisodes\n",
    "            for a in range(max_actions):\n",
    "                action = self._policy('train', state, exploration_rate)\n",
    "                new_state, reward, done, info = self.env.step(action)\n",
    "                # The formulation of updating Q(s, a)\n",
    "                self.Q[state, action] = self.Q[state, action] + learning_rate*(reward+discount*np.max(self.Q[new_state, :]) -\n",
    "                                                                               self.Q[state, action])\n",
    "                state = new_state # update the current state\n",
    "                if done == True:  # if fall in the hole or arrive to the goal, then this episode is terminated.\n",
    "                    break\n",
    "            if exploration_rate>0.001:\n",
    "                exploration_rate -= exploration_decay\n",
    "                \n",
    "    def test(self):\n",
    "        # Setting hyper-parameters\n",
    "        max_actions = self.max_actions\n",
    "        state = self.env.reset() # reset the environment\n",
    "        for a in range(max_actions):\n",
    "            self.env.render() # show the environment states\n",
    "            action = np.argmax(self.Q[state,:]) # take action with the Optimal Policy\n",
    "            new_state, reward, done, info = self.env.step(action) # arrive to next_state after taking the action\n",
    "            state = new_state # update current state\n",
    "            if done:\n",
    "                print(\"======\")\n",
    "                self.env.render()\n",
    "                break\n",
    "            print(\"======\")\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING...\n",
      "\n",
      "==============\n",
      "\n",
      "TEST==============\n",
      "\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "======\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "======\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "======\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "======\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "======\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "======\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = easydict.EasyDict({\n",
    "        \"max_episodes\" : 200,\n",
    "        \"max_actions\" : 99,\n",
    "        \"learning_rate\" : 0.83,\n",
    "        \"discount\" : 0.95,\n",
    "        \"exploration_rate\" : 1.0\n",
    "    })\n",
    "    env = Environment().FrozenLakeNoSlippery() # construct the environment\n",
    "    agent = QAgent(args, env) # get agent\n",
    "    print(\"START TRAINING...\")\n",
    "    agent.train()\n",
    "    print(\"\\n==============\\n\\nTEST==============\\n\")\n",
    "    agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
